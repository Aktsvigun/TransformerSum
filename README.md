# TransformerSum
> A model to perform neural summarization (extractive and abstractive) using machine learning transformer models and a tool to convert abstractive summarization datasets to the extractive task.

## Pre-trained Models

### CNN/DM

| Name | Comments | Model Download | Data Download |
|---------------------------------|----------|----------------|-----------------------------------------------------------------------------------------|
| distilbert-base-uncased-ext-sum | None | Not yet... | [CNN/DM Bert Uncased](https://drive.google.com/uc?id=1i1G7dEBsJ1ZIuty-wJrEGqbzdLvZCnV9) |
| distilroberta-base-ext-sum | None | Not yet... | [CNN/DM Roberta](https://drive.google.com/uc?id=1g2bAAUuDvt3WZ3giSDaN7j4xew-RGfT3) |
| bert-base-uncased-ext-sum | None | Not yet... | [CNN/DM Bert Uncased](https://drive.google.com/uc?id=1i1G7dEBsJ1ZIuty-wJrEGqbzdLvZCnV9) |
| roberta-base-ext-sum | None | Not yet... | [CNN/DM Roberta](https://drive.google.com/uc?id=1g2bAAUuDvt3WZ3giSDaN7j4xew-RGfT3) |
| albert-base-v2-ext-sum | None | Not yet... | [CNN/DM Albert](https://drive.google.com/uc?id=1MPWFtcauXtzEVPDXZTA7o0MWi47GuhFU) |
| bert-large-uncased-ext-sum | None | Not yet... | [CNN/DM Bert Uncased](https://drive.google.com/uc?id=1i1G7dEBsJ1ZIuty-wJrEGqbzdLvZCnV9) |
| roberta-large-ext-sum | None | Not yet... | [CNN/DM Roberta](https://drive.google.com/uc?id=1g2bAAUuDvt3WZ3giSDaN7j4xew-RGfT3) |
| albert-xlarge-v2-ext-sum | None | Not yet... | [CNN/DM Albert](https://drive.google.com/uc?id=1MPWFtcauXtzEVPDXZTA7o0MWi47GuhFU) |

#### CNN/DM ROUGE Scores

Test set results on the CNN/DailyMail dataset using ROUGE F<sub>1</sub>.

| Name                            | ROUGE-1    | ROUGE-2    | ROUGE-L    |
|---------------------------------|------------|------------|------------|
| distilbert-base-uncased-ext-sum | Not yet... | Not yet... | Not yet... |
| distilroberta-base-ext-sum      | Not yet... | Not yet... | Not yet... |
| bert-base-uncased-ext-sum       | Not yet... | Not yet... | Not yet... |
| roberta-base-ext-sum            | Not yet... | Not yet... | Not yet... |
| albert-base-v2-ext-sum          | Not yet... | Not yet... | Not yet... |
| bert-large-uncased-ext-sum      | Not yet... | Not yet... | Not yet... |
| roberta-large-ext-sum           | Not yet... | Not yet... | Not yet... |
| albert-xlarge-v2-ext-sum        | Not yet... | Not yet... | Not yet... |

### WikiHow

| Name | Comments | Model Download | Data Download |
|---------------------------------|----------|----------------|--------------------------|
| distilbert-base-uncased-ext-sum | None | Not yet... | [WikiHow Bert Uncased]() |
| distilroberta-base-ext-sum | None | Not yet... | [WikiHow Roberta]() |
| bert-base-uncased-ext-sum | None | Not yet... | [WikiHow Bert Uncased]() |
| roberta-base-ext-sum | None | Not yet... | [WikiHow Roberta]() |
| albert-base-v2-ext-sum | None | Not yet... | [WikiHow Albert]() |
| bert-large-uncased-ext-sum | None | Not yet... | [WikiHow Bert Uncased]() |
| roberta-large-ext-sum | None | Not yet... | [WikiHow Roberta]() |
| albert-xlarge-v2-ext-sum | None | Not yet... | [WikiHow Albert]() |

#### WikiHow ROUGE Scores

Test set results on the WikiHow dataset using ROUGE F<sub>1</sub>.

| Name                            | ROUGE-1    | ROUGE-2    | ROUGE-L    |
|---------------------------------|------------|------------|------------|
| distilbert-base-uncased-ext-sum | Not yet... | Not yet... | Not yet... |
| distilroberta-base-ext-sum      | Not yet... | Not yet... | Not yet... |
| bert-base-uncased-ext-sum       | Not yet... | Not yet... | Not yet... |
| roberta-base-ext-sum            | Not yet... | Not yet... | Not yet... |
| albert-base-v2-ext-sum          | Not yet... | Not yet... | Not yet... |
| bert-large-uncased-ext-sum      | Not yet... | Not yet... | Not yet... |
| roberta-large-ext-sum           | Not yet... | Not yet... | Not yet... |
| albert-xlarge-v2-ext-sum        | Not yet... | Not yet... | Not yet... |

### arXiv-PubMed

| Name | Comments | Model Download | Data Download |
|---------------------------------|----------|----------------|--------------------------|
| distilbert-base-uncased-ext-sum | None | Not yet... | [arXiv-PubMed Bert Uncased](https://drive.google.com/uc?id=1-htznO-Io6r-9rVSTMQ1-4HYhyu21w7s) |
| distilroberta-base-ext-sum | None | Not yet... | [arXiv-PubMed Roberta]() |
| bert-base-uncased-ext-sum | None | Not yet... | [arXiv-PubMed Bert Uncased]() |
| roberta-base-ext-sum | None | Not yet... | [arXiv-PubMed Roberta]() |
| albert-base-v2-ext-sum | None | Not yet... | [arXiv-PubMed Albert]() |
| bert-large-uncased-ext-sum | None | Not yet... | [arXiv-PubMed Bert Uncased]() |
| roberta-large-ext-sum | None | Not yet... | [arXiv-PubMed Roberta]() |
| albert-xlarge-v2-ext-sum | None | Not yet... | [arXiv-PubMed Albert]() |

#### arXiv-PubMed ROUGE Scores

Test set results on the arXiv-PubMed dataset using ROUGE F<sub>1</sub>.

| Name                            | ROUGE-1    | ROUGE-2    | ROUGE-L    |
|---------------------------------|------------|------------|------------|
| distilbert-base-uncased-ext-sum | Not yet... | Not yet... | Not yet... |
| distilroberta-base-ext-sum      | Not yet... | Not yet... | Not yet... |
| bert-base-uncased-ext-sum       | Not yet... | Not yet... | Not yet... |
| roberta-base-ext-sum            | Not yet... | Not yet... | Not yet... |
| albert-base-v2-ext-sum          | Not yet... | Not yet... | Not yet... |
| bert-large-uncased-ext-sum      | Not yet... | Not yet... | Not yet... |
| roberta-large-ext-sum           | Not yet... | Not yet... | Not yet... |
| albert-xlarge-v2-ext-sum        | Not yet... | Not yet... | Not yet... |

## Install

Installation is made easy due to conda environments. Simply run this command from the root project directory: `conda env create --file environment.yml` and conda will create and environment called `transformersum` with all the required packages from [environment.yml](environment.yml). The spacy `en_core_web_sm` model is required for the [convert_to_extractive.py](convert_to_extractive.py) script to detect sentence boundaries.

### Step-by-Step Instructions

1. Clone this repository: `git clone https://github.com/HHousen/transformersum.git`.
2. Change to project directory: `cd transformersum`.
3. Run installation command: `conda env create --file environment.yml`.
4. **(Optional)** If using the [convert_to_extractive.py](convert_to_extractive.py) script then download the `en_core_web_sm` spacy model: `python -m spacy download en_core_web_sm`.

## Supported Datasets

Currently only the CNN/DM summarization dataset is supported.

There are several ways to obtain and process the datasets below:

1. Download the converted extractive version for use with the training script (which will preprocess the data automatically (tokenization, etc.)). Note that all the provided extractive versions are split every 500 documents and are compressed. You will have to manually process if you desire different chunk sizes.
2. Download the processed abstractive version. This is the original data after begin run through its respective processor located in `datasets`.
3. Download the original data in its original form, which depends on how it was obtained in the original paper.

The table under each heading contains quick links to download the data. Beneath that are instructions to process the data manually.

### CNN/DM

The **CNN/DailyMail** (Hermann et al., 2015) dataset contains 93k articles from the CNN, and 220k articles the Daily Mail newspapers. Both publishers supplement their articles with bullet point summaries. Non-anonymized variant in See et al. (2017).

| Type | Link |
|-------------------------------|----------------------------------------------------------------------------------|
| Processor Repository | [artmatsak/cnn-dailymail](https://github.com/artmatsak/cnn-dailymail) |
| Data Download Link | [CNN/DM official website](https://cs.nyu.edu/~kcho/DMQA/) |
| Processed Abstractive Dataset | [Google Drive](https://drive.google.com/uc?id=1OMJWMoO367yPZH5yp-TsAai_kUmWVQY2) |
| Extractive Version | [Google Drive](https://drive.google.com/uc?id=1_nmp6nzbiW2HUEtJPXn8WzF2XhMVem2n) |

Download and unzip the stories directories from [here](https://cs.nyu.edu/~kcho/DMQA/) for both CNN and Daily Mail. The files can be downloaded from the terminal with `gdown`, which can be installed with `pip install gdown`.

```
pip install gdown
gdown https://drive.google.com/uc?id=0BwmD_VLjROrfTHk4NFg2SndKcjQ
gdown https://drive.google.com/uc?id=0BwmD_VLjROrfM1BxdkxVaTY2bWs
tar zxf cnn_stories.tgz
tar zxf dailymail_stories.tgz
```

**Note:** The above Google Drive links may be outdated depending on the time you are reading this. Check the [CNN/DM official website](https://cs.nyu.edu/~kcho/DMQA/) for the most up-to-date download links.

Next, run the processing code in the git submodule for [artmatsak/cnn-dailymail](https://github.com/artmatsak/cnn-dailymail) located in `datasets/cnn_dailymail_processor`. Run `python make_datafiles.py /path/to/cnn/stories /path/to/dailymail/stories`, replacing `/path/to/cnn/stories` with the path to where you saved the `cnn/stories` directory that you downloaded; similarly for `dailymail/stories`.

For each of the URL lists (`all_train.txt`, `all_val.txt` and `all_test.txt`) in `cnn_dailymail_processor/url_lists`, the corresponding stories are read from file and written to text files `train.source`, `train.target`, `val.source`, `val.target`, and `test.source` and `test.target`. These will be placed in the newly created `cnn_dm` directory.

The original processing code is available at [abisee/cnn-dailymail](https://github.com/abisee/cnn-dailymail), but for this project the [artmatsak/cnn-dailymail](https://github.com/artmatsak/cnn-dailymail) processing code is used since it does not tokenize and writes the data to text file `train.source`, `train.target`, `val.source`, `val.target`, `test.source` and `test.target`, which is the format expected by [convert_to_extractive.py](convert_to_extractive.py).

### WikiHow

**WikiHow** (Koupaee and Wang, 2018) is a large-scale dataset of instructions from the online WikiHow.com website. Each of 200k examples consists of multiple instruction-step paragraphs along with a summarizing sentence. The task is to generate the concatenated summary-sentences from the paragraphs.

| Dataset Size           | 230,843 |
|------------------------|---------|
| Average Article Length | 579.8   |
| Average Summary Length | 62.1    |
| Vocabulary Size        | 556,461 |

| Type | Link |
|-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------|
| Processor Repository | [HHousen/WikiHow-Dataset](https://github.com/HHousen/WikiHow-Dataset) ([Original Repo](https://github.com/mahnazkoupaee/WikiHow-Dataset)) |
| Data Download Link | [wikihowAll.csv](https://bit.ly/3cueodA) ([mirror](https://drive.google.com/uc?id=1_Xps_EI-S2Y6V785iKWtj3SLjjNHOaPc)) and [wikihowSep.csv](https://bit.ly/3btJ12G) |
| Processed Abstractive Dataset | [Google Drive](https://drive.google.com/uc?id=1KZkRW3WNUCMIzYubiYQzTRs8G5wgqfvN) |
| Extractive Version | [Google Drive](https://drive.google.com/uc?id=1-0FqGVWiXfwnQjCW7WLvRipl6-Z4gvXn) |

Processing Steps:

1. Download [wikihowAll.csv](https://bit.ly/3cueodA) ([main repo](https://github.com/mahnazkoupaee/WikiHow-Dataset) for most up-to-date links) to `datasets/wikihow_processor`
2. Run `python process.py` (runtime: 2m), which will create a new directory called `wikihow` containing the `train.source`, `train.target`, `val.source`, `val.target`, `test.source` and `test.target` files necessary for [convert_to_extractive.py](convert_to_extractive.py).

### PubMed/ArXiv

**ArXiv and PubMed** (Cohan et al., 2018) are two long document datasets of scientific publications
from [arXiv.org](http://arxiv.org/) (113k) and PubMed (215k). The task is to generate the abstract from the paper body.

| Datasets              | # docs | avg. doc. length (words) | avg. summary length (words) |
|-----------------------|--------|--------------------------|-----------------------------|
| CNN                   | 92K    | 656                      | 43                          |
| Daily Mail            | 219K   | 693                      | 52                          |
| NY Times              | 655K   | 530                      | 38                          |
| PubMed (this dataset) | 133K   | 3016                     | 203                         |
| arXiv (this dataset)  | 215K   | 4938                     | 220                         |

| Type | Link |
|-------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------|
| Processor Repository | [HHousen/ArXiv-PubMed-Sum](https://github.com/HHousen/ArXiv-PubMed-Sum) ([Original Repo](https://github.com/armancohan/long-summarization)) |
| Data Download Link | [PubMed](https://bit.ly/2VsKNvt) ([mirror](https://bit.ly/2VLPJuh)) and [ArXiv](https://bit.ly/2wWeVpp) ([mirror](https://bit.ly/2VPWnzs)) |
| Processed Abstractive Dataset | [Google Drive](https://drive.google.com/uc?id=1-fTvAvgimuBFVgwOEkQUkURkBoIS5XlL) |
| Extractive Version | [Google Drive](https://drive.google.com/uc?id=12tdPhuwNsofQJSkpcwbFRYYr4euMrd7j). |

Processing Steps:

1. Download [PubMed](https://bit.ly/2VsKNvt) and [ArXiv](https://bit.ly/2wWeVpp) ([main repo](https://github.com/armancohan/long-summarization) for most up-to-date links) to `datasets/arxiv-pubmed_processor`
2. Run the command `python process.py <arxiv_articles_dir> <pubmed_articles_dir>` (runtime: 5-10m), which will create a new directory called `arxiv-pubmed` containing the `train.source`, `train.target`, `val.source`, `val.target`, `test.source` and `test.target` files necessary for [convert_to_extractive.py](convert_to_extractive.py).

See the [repository's README.md](datasets/arxiv-pubmed_processor/README.md) for more information.

Note to convert this dataset to extractive it is recommended to use the `--sentencizer` option due to the size of the dataset. Additionally, the `--max_sentence_ntokens` should be set to `300` and the `--max_example_nsents` should be set to `600`. See the "Convert Abstractive to Extractive Dataset" section below. The full command should be similar to:

```
python convert_to_extractive.py ./datasets/arxiv-pubmed_processor/arxiv-pubmed \
--shard_interval 5000 \
--sentencizer \
--max_sentence_ntokens 300 \
--max_example_nsents 600
```

## Convert Abstractive to Extractive Dataset

Simply run [convert_to_extractive.py](convert_to_extractive.py) with the path to the data. For example, with the CNN/DM dataset downloaded above: `python convert_to_extractive.py ./datasets/cnn_dailymail_processor/cnn_dm`. However, the recommended command is: `python convert_to_extractive.py ./datasets/cnn_dailymail_processor/cnn_dm --shard_interval 5000 --compression --add_target_to test`, the `--shard_interval` processes the file in chunks of `5000` and writes results to disk in chunks of `5000` (saves RAM) and the `--compression` compresses each output chunk with gzip (depending on the dataset reduces space usage requirement by about 1/2 to 1/3). The default output directory is the input directory that was specified, but the output directory can be changed with `--base_output_path` if desired.

The `--add_target_to` argument will save the abstractive target text to the splits (in `--split_names`) specified.

If your files are not `train`, `val`, and `test`, then the `--split_names` argument will let you specify the correct naming pattern. The `--source_ext` and `--target_ext` let you specify the file extension of the source and target files respectively. These must be different so the process can tell each section apart.

**Large Dataset? Need to Resume?:** The `--resume` option will read the output directory and determine on which document the script left off based on the shard_file names. If `--shard_interval` was `None` then resuming is not possible. Resuming is guaranteed to produce the same output as if `--resume` was not used because of `check_resume_success()`, which checks to make sure the last line in the shard file is the same as the line directly before the line to resume with.

**Speed: Running Slowly?** There is a `--sentencizer` option to detect sentence boundaries without parsing dependencies. Instead of loading a statistical model using `spacy`, this option will initialize the `English` [Language](https://spacy.io/api/language#init) object and add a `sentencizer` to the [pipeline](https://spacy.io/api/language#create_pipe). This is much faster than a [DependencyParser](https://spacy.io/api/dependencyparser) but is also less accurate since the `sentencizer` uses a simpler, rule-based strategy.

### Script Help

Output of `python convert_to_extractive.py --help`:

```
usage: convert_to_extractive.py [-h] [--base_output_path BASE_OUTPUT_PATH]
                                [--split_names {train,val,test} [{train,val,test} ...]]
                                [--add_target_to {train,val,test} [{train,val,test} ...]]
                                [--source_ext SOURCE_EXT]
                                [--target_ext TARGET_EXT]
                                [--oracle_mode {greedy,combination}]
                                [--shard_interval SHARD_INTERVAL]
                                [--n_process N_PROCESS]
                                [--batch_size BATCH_SIZE] [--compression]
                                [--resume]
                                [--tokenizer_log_interval TOKENIZER_LOG_INTERVAL]
                                [--sentencizer] [--no_preprocess]
                                [--min_sentence_ntokens MIN_SENTENCE_NTOKENS]
                                [--max_sentence_ntokens MAX_SENTENCE_NTOKENS]
                                [--min_example_nsents MIN_EXAMPLE_NSENTS]
                                [--max_example_nsents MAX_EXAMPLE_NSENTS]
                                [-l {DEBUG,INFO,WARNING,ERROR,CRITICAL}]
                                DIR

Convert an Abstractive Summarization Dataset to the Extractive Task

positional arguments:
  DIR                   path to data directory

optional arguments:
  -h, --help            show this help message and exit
  --base_output_path BASE_OUTPUT_PATH
                        path to output processed data (default is `base_path`)
  --split_names {train,val,test} [{train,val,test} ...]
                        which splits of dataset to process
  --add_target_to {train,val,test} [{train,val,test} ...]
                        add the abstractive target to these splits (useful for
                        calculating rouge scores)
  --source_ext SOURCE_EXT
                        extension of source files
  --target_ext TARGET_EXT
                        extension of target files
  --oracle_mode {greedy,combination}
                        method to convert abstractive summaries to extractive
                        summaries
  --shard_interval SHARD_INTERVAL
                        how many examples to include in each shard of the
                        dataset (default: no shards)
  --n_process N_PROCESS
                        number of processes for multithreading
  --batch_size BATCH_SIZE
                        number of batches for tokenization
  --compression         use gzip compression when saving data
  --resume              resume from last shard
  --tokenizer_log_interval TOKENIZER_LOG_INTERVAL
                        minimum progress display update interval [default:
                        0.1] seconds
  --sentencizer         use a spacy sentencizer instead of a statistical model
                        for sentence detection (much faster but less
                        accurate); see https://spacy.io/api/sentencizer
  --no_preprocess       do not run the preprocess function, which removes
                        sentences that are too long/short and examples that
                        have too few/many sentences
  --min_sentence_ntokens MIN_SENTENCE_NTOKENS
                        minimum number of tokens per sentence
  --max_sentence_ntokens MAX_SENTENCE_NTOKENS
                        maximum number of tokens per sentence
  --min_example_nsents MIN_EXAMPLE_NSENTS
                        minimum number of sentences per example
  --max_example_nsents MAX_EXAMPLE_NSENTS
                        maximum number of sentences per example
  -l {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --log {DEBUG,INFO,WARNING,ERROR,CRITICAL}
                        Set the logging level (default: 'Info').
```

## Training an Extractive Summarization Model

Once the dataset has been converted to the extractive task, it can be used as input to a SentencesProcessor, which has a `add_examples()` function too add sets of `(example, labels)` and a `get_features()` function that returns a TensorDataset of extracted features (`input_ids`, `attention_masks`, `labels`, `token_type_ids`, `sent_rep_token_ids`, `sent_rep_token_ids_masks`). Feature extraction runs in parallel and tokenizes text using the tokenizer appropriate for the model specified with `--model_name_or_path`. The tokenizer can be changed to another huggingface/transformers tokenizer with the `--tokenizer_name` option. 

Continuing with the CNN/CM dataset, to train a model for 50,000 steps on the data run: `python main.py --data_path ./datasets/cnn_dailymail_processor/cnn_dm --default_save_path ./trained_models --do_train --max_steps 50000`.

The `--do_train` argument runs the training process. Set `--do_test` to test after training.
The `--data_path` argument specifies where the extractive dataset json file are located.
The `--default_save_path` argument specifies where the logs and model weights should be stored.
If you prefer to measure training progress by epochs instead of steps, the `--max_epochs` and `--min_epochs` options exist just for you.

The batch size can be changed with the `--batch_size` option. This changes the batch size for training, validation, and testing. You can set the `--auto_scale_batch_size` option to automatically determine this value. See ["Auto scaling of batch size" from the pytorch_lightning documentation](https://pytorch-lightning.readthedocs.io/en/0.7.6/training_tricks.html#auto-scaling-of-batch-size) for more information about the algorithm and available options.

If the extractive dataset json files are compressed using json, then they will be automatically decompressed during the data preprocessing step of training.

By default the model is saved after every epoch to the `--default_save_path`.

### Automatic Preprocessing

While the [convert_to_extractive.py](convert_to_extractive.py) script prepares a dataset for the extractive task, the data still needs to be processed for usage with a machine learning model. This preprocessing depends on the chosen model, and thus is implemented in the [extractive.py](extractive.py) file.

The actual ExtractiveSummarizer LightningModule (which is similar to an nn.Module but with a built-in training loop, more info at the [pytorch_lightning documentation](https://pytorch-lightning.readthedocs.io/en/latest/)) implements a `prepare_data()` function. This `prepare_data()` function is automatically called by `pytorch_lightning` to load and process the examples.

Memory Usage Note: If sharding was turned off during the `convert_to_extractive` process then `prepare_data()` will run once, loading the entire dataset into memory to process just like the [convert_to_extractive.py](convert_to_extractive.py) script.

There is a `--only_preprocess` argument available to only run this preprocess step and exit the script after all the examples have been written to disk. This option will force data to be preprocessed, even if it was already computed and is detected on disk, and any previous processed files will be overwritten.

Thus, the command to only preprocess data for use when training a model run: `python main.py --data_path ./datasets/cnn_dailymail_processor/cnn_dm --use_logger tensorboard --model_name_or_path bert-base-uncased --model_type bert --do_train --only_preprocess`

**Important Note:** If processed files are detected, they will automatically be loaded from disk. This includes any files that follow the pattern `[dataset_split_name].*.pt`, where `*` is any text of any length.

### Pooling Modes

The pooling model determines how word vectors should be converted to sentence embeddings. The implementation can be found in [pooling.py](pooling.py). The `--pooling_mode` argument can be set to either `sent_rep_tokens` or `mean_tokens`. While the [pooling nn.Module](pooling.py) allows multiple methods to be used at once (it will concatenate and return the results), the training script does not.

* `sent_rep_tokens`: Use the sentence representation token vectors as sentence embeddings.
* `mean_tokens`: Take the mean of all the token vectors in each sentence.

### Custom Models

You can use any transformer model for the word embedding model as long as it was saved in the `huggingface/transformers` format with the `--model_name_or_path` CLI argument. Any model that is loaded with this option by specifying a path is considered "custom" in this project. Currently, there are no "custom" models that are "officially" supported. The `longformer` used to be a custom model, but it was since added to the `huggingface/transformers` repository, and thus can be used in this project just like any other model.

### Script Help

Output of `python main.py --help`:

```
usage: main.py [-h] [--default_save_path DEFAULT_SAVE_PATH] [--learning_rate LEARNING_RATE] [--min_epochs MIN_EPOCHS]
                [--max_epochs MAX_EPOCHS] [--min_steps MIN_STEPS] [--max_steps MAX_STEPS] [--accumulate_grad_batches ACCUMULATE_GRAD_BATCHES]
                [--check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH] [--gpus GPUS] [--gradient_clip_val GRADIENT_CLIP_VAL]
                [--overfit_pct OVERFIT_PCT] [--train_percent_check TRAIN_PERCENT_CHECK] [--val_percent_check VAL_PERCENT_CHECK]
                [--test_percent_check TEST_PERCENT_CHECK] [--amp_level AMP_LEVEL] [--precision PRECISION] [--seed SEED] [--profiler]
                [--progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE] [--num_sanity_val_steps NUM_SANITY_VAL_STEPS]
                [--use_logger {tensorboard,wandb}] [--do_train] [--do_test] [--load_weights LOAD_WEIGHTS]
                [--load_from_checkpoint LOAD_FROM_CHECKPOINT] [--use_custom_checkpoint_callback]
                [--custom_checkpoint_every_n CUSTOM_CHECKPOINT_EVERY_N]
                [--custom_checkpoint_every_n_save_path CUSTOM_CHECKPOINT_EVERY_N_SAVE_PATH] [--no_wandb_logger_log_model]
                [-l {DEBUG,INFO,WARNING,ERROR,CRITICAL}] [--model_name_or_path MODEL_NAME_OR_PATH] [--model_type MODEL_TYPE]
                [--tokenizer_name TOKENIZER_NAME] [--tokenizer_lowercase] [--max_seq_length MAX_SEQ_LENGTH] [--data_path DATA_PATH]
                [--num_threads NUM_THREADS] [--processing_num_threads PROCESSING_NUM_THREADS] [--weight_decay WEIGHT_DECAY]
                [--pooling_mode {sent_rep_tokens,mean_tokens}] [--adam_epsilon ADAM_EPSILON] [--optimizer_type OPTIMIZER_TYPE]
                [--ranger-k RANGER_K] [--warmup_steps WARMUP_STEPS] [--use_scheduler USE_SCHEDULER] [--num_frozen_steps NUM_FROZEN_STEPS]
                [--train_batch_size TRAIN_BATCH_SIZE] [--val_batch_size VAL_BATCH_SIZE] [--test_batch_size TEST_BATCH_SIZE]
                [--processor_no_bert_compatible_cls] [--only_preprocess] [--preprocess_resume] [--create_token_type_ids {binary,sequential}]
                [--no_use_token_type_ids] [--classifier {linear,transformer,transformer_linear}] [--classifier_dropout CLASSIFIER_DROPOUT]
                [--classifier_transformer_num_layers CLASSIFIER_TRANSFORMER_NUM_LAYERS] [--train_name TRAIN_NAME] [--val_name VAL_NAME]
                [--test_name TEST_NAME] [--test_id_method {greater_k,top_k}] [--test_k TEST_K]
                [--loss_key {loss_total,loss_total_norm_batch,loss_avg_seq_sum,loss_avg_seq_mean,loss_avg}]

  optional arguments:
    -h, --help            show this help message and exit
    --default_save_path DEFAULT_SAVE_PATH
                          Default path for logs and weights. To use this option with the `wandb` logger specify the
                          `--no_wandb_logger_log_model` option.
    --learning_rate LEARNING_RATE
                          The initial learning rate for the optimizer.
    --min_epochs MIN_EPOCHS
                          Limits training to a minimum number of epochs
    --max_epochs MAX_EPOCHS
                          Limits training to a max number number of epochs
    --min_steps MIN_STEPS
                          Limits training to a minimum number number of steps
    --max_steps MAX_STEPS
                          Limits training to a max number number of steps
    --accumulate_grad_batches ACCUMULATE_GRAD_BATCHES
                          Accumulates grads every k batches. A single step is one gradient accumulation cycle, so setting this value to 2 will
                          cause 2 batches to be processed for each step.
    --check_val_every_n_epoch CHECK_VAL_EVERY_N_EPOCH
                          Check val every n train epochs.
    --gpus GPUS           Number of GPUs to train on or Which GPUs to train on. (default: -1 (all gpus))
    --gradient_clip_val GRADIENT_CLIP_VAL
                          Gradient clipping value
    --overfit_pct OVERFIT_PCT
                          Uses this much data of all datasets (training, validation, test). Useful for quickly debugging or trying to overfit
                          on purpose.
    --train_percent_check TRAIN_PERCENT_CHECK
                          How much of training dataset to check. Useful when debugging or testing something that happens at the end of an
                          epoch.
    --val_percent_check VAL_PERCENT_CHECK
                          How much of validation dataset to check. Useful when debugging or testing something that happens at the end of an
                          epoch.
    --test_percent_check TEST_PERCENT_CHECK
                          How much of test dataset to check.
    --amp_level AMP_LEVEL
                          The optimization level to use (O1, O2, etc…) for 16-bit GPU precision (using NVIDIA apex under the hood).
    --precision PRECISION
                          Full precision (32), half precision (16). Can be used on CPU, GPU or TPUs.
    --seed SEED           Seed for reproducible results. Can negatively impact performace in some cases.
    --profiler            To profile individual steps during training and assist in identifying bottlenecks.
    --progress_bar_refresh_rate PROGRESS_BAR_REFRESH_RATE
                          How often to refresh progress bar (in steps). In notebooks, faster refresh rates (lower number) is known to crash
                          them because of their screen refresh rates, so raise it to 50 or more.
    --num_sanity_val_steps NUM_SANITY_VAL_STEPS
                          Sanity check runs n batches of val before starting the training routine. This catches any bugs in your validation
                          without having to wait for the first validation check.
    --use_logger {tensorboard,wandb}
                          Which program to use for logging. If `--use_custom_checkpoint_callback` is specified and `wandb` is chosen then model
                          weights will automatically be uploaded to wandb.ai.
    --do_train            Run the training procedure.
    --do_test             Run the testing procedure.
    --load_weights LOAD_WEIGHTS
                          Loads the model weights from a given checkpoint
    --load_from_checkpoint LOAD_FROM_CHECKPOINT
                          Loads the model weights and hyperparameters from a given checkpoint.
    --use_custom_checkpoint_callback
                          Use the custom checkpointing callback specified in main() by `args.checkpoint_callback`. By default this custom
                          callback saves the model every epoch and never deletes and saved weights files. Set this option and `--use_logger` to
                          `wandb` to automatically upload model weights to wandb.ai. DO NOT set this and `--user_logger` to "tensorboard"
                          because a custom TensorBoardLogger is not created. Thus, when the trainer attempts to save the model, the program
                          will crash since `--default_save_path` is set and a custom checkpoint callback is passed. See: https://pytorch-
                          lightning.readthedocs.io/en/latest/trainer.html#default-root-dir ("Default path for logs and weights when **no logger
                          or ModelCheckpoint callback** passed.")
    --custom_checkpoint_every_n CUSTOM_CHECKPOINT_EVERY_N
                          The number of steps between additional checkpoints. By default checkpoints are saved every epoch. Setting this value
                          will save them every epoch and every N steps. This does not use the same callback as
                          `--use_custom_checkpoint_callback` but instead uses a different class called `StepCheckpointCallback`.
    --custom_checkpoint_every_n_save_path CUSTOM_CHECKPOINT_EVERY_N_SAVE_PATH
                          Path to save models when using `--custom_checkpoint_every_n`.
    --no_wandb_logger_log_model
                          Only applies when using the `wandb` logger. Set this argument to NOT save checkpoints in wandb directory to upload to
                          W&B servers.
    -l {DEBUG,INFO,WARNING,ERROR,CRITICAL}, --log {DEBUG,INFO,WARNING,ERROR,CRITICAL}
                          Set the logging level (default: 'Info').
    --model_name_or_path MODEL_NAME_OR_PATH
                          Path to pre-trained model or shortcut name. A list of shortcut names can be found at https://huggingface.co/models.
    --model_type MODEL_TYPE
                          Model type selected in the list: t5, distilbert, albert, camembert, xlm-roberta, bart, longformer, roberta, bert,
                          openai-gpt, gpt2, transfo-xl, xlnet, flaubert, xlm, ctrl, electra, reformer
    --tokenizer_name TOKENIZER_NAME
    --tokenizer_lowercase
    --max_seq_length MAX_SEQ_LENGTH
    --data_path DATA_PATH
                          Directory containing the dataset.
    --num_threads NUM_THREADS
    --processing_num_threads PROCESSING_NUM_THREADS
    --weight_decay WEIGHT_DECAY
    --pooling_mode {sent_rep_tokens,mean_tokens}
                          How word vectors should be converted to sentence embeddings.
    --adam_epsilon ADAM_EPSILON
                          Epsilon for Adam optimizer.
    --optimizer_type OPTIMIZER_TYPE
                          Which optimizer to use: 1. `ranger` optimizer (combination of RAdam and LookAhead) 2. `adamw` 3. `yellowfin`
    --ranger-k RANGER_K   Ranger (LookAhead) optimizer k value (default: 6). LookAhead keeps a single extra copy of the weights, then lets the
                          internalized ‘faster’ optimizer (for Ranger, that’s RAdam) explore for 5 or 6 batches. The batch interval is
                          specified via the k parameter.
    --warmup_steps WARMUP_STEPS
                          Linear warmup over warmup_steps. Only active if `--use_scheduler` is set.
    --use_scheduler USE_SCHEDULER
                          Two options: 1. `linear`: Use a linear schedule that inceases linearly over `--warmup_steps` to `--learning_rate`
                          then decreases linearly for the rest of the training process. 2. `onecycle`: Use the one cycle policy with a maximum
                          learning rate of `--learning_rate`. (default: False, don't use any scheduler)
    --num_frozen_steps NUM_FROZEN_STEPS
                          Freeze (don't train) the word embedding model for this many steps.
    --train_batch_size TRAIN_BATCH_SIZE
                          Batch size per GPU/CPU for training.
    --val_batch_size VAL_BATCH_SIZE
                          Batch size per GPU/CPU for evaluation.
    --test_batch_size TEST_BATCH_SIZE
                          Batch size per GPU/CPU for testing.
    --processor_no_bert_compatible_cls
                          If model uses bert compatible [CLS] tokens for sentence representations.
    --only_preprocess     Only preprocess and write the data to disk. Don't train model. This will force data to be preprocessed, even if it
                          was already computed and is detected on disk, and any previous processed files will be overwritten.
    --preprocess_resume   Resume preprocessing. `--only_preprocess` must be set in order to resume. Determines which files to process by
                          finding the shards that do not have a coresponding ".pt" file in the data directory.
    --create_token_type_ids {binary,sequential}
                          Create token type ids during preprocessing.
    --no_use_token_type_ids
                          Set to not train with `token_type_ids` (don't pass them into the model).
    --classifier {linear,transformer,transformer_linear}
                          Which classifier/encoder to use to reduce the hidden dimension of the sentence vectors. `linear` - a
                          `LinearClassifier` with two linear layers, dropout, and an activation function. `transformer` - a
                          `TransformerEncoderClassifier` which runs the sentence vectors through some `nn.TransformerEncoderLayer`s and then a
                          simple `nn.Linear` layer. `transformer_linear` - a `TransformerEncoderClassifier` with a `LinearClassifier` as the
                          `reduction` parameter, which results in the same thing as the `transformer` option but with a `LinearClassifier`
                          instead of a `nn.Linear` layer.
    --classifier_dropout CLASSIFIER_DROPOUT
                          The value for the dropout layers in the classifier.
    --classifier_transformer_num_layers CLASSIFIER_TRANSFORMER_NUM_LAYERS
                          The number of layers for the `transformer` classifier. Only has an effect if `--classifier` contains "transformer".
    --train_name TRAIN_NAME
                          name for set of training files on disk (for loading and saving)
    --val_name VAL_NAME   name for set of validation files on disk (for loading and saving)
    --test_name TEST_NAME
                          name for set of testing files on disk (for loading and saving)
    --test_id_method {greater_k,top_k}
                          How to chose the top predictions from the model for ROUGE scores.
    --test_k TEST_K       The `k` parameter for the `--test_id_method`. Must be set if using the `greater_k` option. (default: 3)
    --loss_key {loss_total,loss_total_norm_batch,loss_avg_seq_sum,loss_avg_seq_mean,loss_avg}
                          Which reduction method to use with BCELoss. See the `experiments/loss_functions/` folder for info on how the default
                          (`loss_avg_seq_mean`) was chosen.
```

All training arguments can be found in the [pytorch_lightning trainer documentation](https://pytorch-lightning.readthedocs.io/en/latest/trainer.html).

## Experiments

Please see [experiments/README.md](experiments/README.md).

## Difference from BertSum/PreSumm

This project accomplishes a task similar to [BertSum](https://github.com/nlpyang/BertSum)/[PreSumm](https://github.com/nlpyang/PreSumm) (and is based on their research). However, `TransformerSum` improves various aspects and adds several features on top of `BertSum`/`PreSumm`. The most notable improvements include:

Note: PreSumm (Yang Liu and Mirella Lapata) builds on top of BertSum (Yang Liu) by adding abstractive summarization.

**General**

* This project uses `pytorch_lighting`, which is a template to better organize PyTorch code. It automates most of the training loop. It also results in easier to read code than plain PyTorch because everything is organized.
* `TransformerSum` contains comments explaining design decisions and how code works. Significant functions also have detailed docstrings. See `get_features()` in [data.py](data.py) for an example (this is the longest docstring in the project).
* Easy pre-trained model loading and modification thanks to `pytorch_lightning`'s `load_from_checkpoint()` that is automatically inherited by every `pl.LightningModule`.

**Converting a Dataset to Extractive**

* Separation between the dataset conversion from abstractive to extractive and the training process. This means the same converted dataset can be used by any transformer model with further automatic processing.
* The [convert_to_extractive.py](convert_to_extractive.py) script uses the more up-to-date `spacy` whereas BertSum utilizes `Stanford CoreNLP`. Spacy has the added benefit of being a python library (while CoreNLP is a java application), which means easier to understand code.
* Supports the same `greedy_selection` and `combination_selection` of `BertSum`.
* A more robust CLI that allows for various desired outputs.
* Built in optional gzip compression.

**Pooling**

* Supports the `sent_rep_tokens` and `mean_tokens` pooling methods whereas `BertSum` only supports `sent_rep_tokens`. See below `Pooling Modes` for more info.
* Pooling is separated from the main model's `forward()` function for easy extendability.

**Data**

* The data processing and loading uses normal PyTorch `DataLoader`s and `Dataset`s instead of recreations of these classes as in `BertSum`.
* A `collate_fn` function converts the data from the dataset to tensors suitable for input to the model, as is stand PyTorch convention.
* The `collate_fn` function also performs "smart batching." It performs padding on the necessary information for each batch, which is more efficient than padding for the entire dataset or each chunk.
* The `FSIterableDataset` class loads a dataset in chunks and is a subclass of `torch.utils.data.IterableDataset`. While `BertSum` also supports chunked loading to lower RAM usage, `TransformerSum`'s technique is more robust and directly integrates with PyTorch.

**Model and Training**

* **Compatible with every `huggingface/transformers` transformer encoder model.** `BertSum` can only use Bert, whereas this project supports all encoders by only changing two options when training.
* Easily extendable with new custom models that are saved in the `huggingface/transformers` format. In this way, integration with the `longformer` was easily accomplished.
* The classifier component of `TransformerSum` is larger (it contains two linear layers) than `BertSum` (which contains one linear layer). The additional layer was found the greatly improve performance.
* The reduction method for the BCE loss function  is different in `TransformerSum` than `BertSum`. `BertSum` takes the sum of the losses for each sentence (ignoring padding) even though it [looks like it uses the mean](https://github.com/nlpyang/BertSum/blob/master/src/models/trainer.py#L325). Five different reduction methods were tested (see the [loss function experiments](experiments/README.md)). There did not appear to a significant difference, but the best was chosen.
* The batch size parameter of `BertSum` is not the real batch size (which is likely caused by the custom `DataLoader`). In this project batch size is the number of documents processed on the GPU at once.
* Multiple optimizers are supported "out-of-the-box" in `TransformerSum` without any need to modify the code.
* The `OneCycle` and `linear_schedule_with_warmup` schedulers are supported in `TransformerSum` "out-of-the-box."
* Logging of all five loss functions (for both the train and validation sets), accuracy, and more is supported. Weights & Biases and Tensorboard are supported "out-of-the-box" but `pytorch_lightning` can integrate several other loggers.

**Where `BertSum` is Better**

* `BertSum` supports three classifiers: a linear layer, a transformer, and a LSTM network. This project supports three different classifiers: a few lienar layers, a transformer, and a linear layer combined with a transformer. The classifier is responsible for removing the hidden features from each sentence embedding and converting them to a single number. However, the [BertSum paper](https://arxiv.org/pdf/1903.10318.pdf) indicates that the difference between these classifiers is not major. `BertSum` has an LSTM classifier, which `TransformerSum` does not replicate.

## Meta

Hayden Housen – [haydenhousen.com](https://haydenhousen.com)

Distributed under the GNU General Public License v3.0. See the [LICENSE](LICENSE) for more information.

<https://github.com/HHousen>

### Attributions

* Code heavily inspired by the following projects:
  * Adapting BERT for Extractive Summariation: [BertSum](https://github.com/nlpyang/BertSum)
  * Word/Sentence Embeddings: [sentence-transformers](https://github.com/UKPLab/sentence-transformers)
  * CNN/CM Dataset: [cnn-dailymail](https://github.com/artmatsak/cnn-dailymail)
  * PyTorch Lightning Classifier: [lightning-text-classification](https://github.com/ricardorei/lightning-text-classification)
* Important projects utilized:
  * PyTorch: [pytorch](https://github.com/pytorch/pytorch/)
  * Training code: [pytorch_lightning](https://github.com/PyTorchLightning/pytorch-lightning/)
  * Transformer Models: [huggingface/transformers](https://github.com/huggingface/transformers)

## Contributing

All Pull Requests are greatly welcomed.

Questions? Commends? Issues? Don't hesitate to open an [issue](https://github.com/HHousen/TransformerSum/issues/new) and briefly describe what you are experiencing (with any error logs if necessary). Thanks.

1. Fork it (<https://github.com/HHousen/TransformerSum/fork>)
2. Create your feature branch (`git checkout -b feature/fooBar`)
3. Commit your changes (`git commit -am 'Add some fooBar'`)
4. Push to the branch (`git push origin feature/fooBar`)
5. Create a new Pull Request
